{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost:\n",
    "    def calc_e(self, X, y, i, threshold, orientation, D):\n",
    "        # 计算错误率和G(x)分类结果\n",
    "        e = np.ones((X.shape[0],1))\n",
    "        Gx = np.zeros((X.shape[0],1))\n",
    "        if orientation == \"left\":\n",
    "            Gx[X[:,i] <= threshold] = 1\n",
    "            Gx[X[:,i] > threshold] = -1\n",
    "        else:\n",
    "            Gx[X[:,i] > threshold] = 1\n",
    "            Gx[X[:,i] <= threshold] = -1\n",
    "        e[Gx == y] = 0\n",
    "        # 加权误差weight_e\n",
    "        weight_e = D * e\n",
    "        return weight_e, Gx\n",
    "\n",
    "    def build_stump(self, X, y, D):\n",
    "        # 设置步长，对于非二值化的数据而言\n",
    "        numSteps = 4\n",
    "        # 用于存储决策树桩的一些数据，比如切点、分类方向、加权误差等\n",
    "        best_stump = {}\n",
    "        weight_e_min = 1\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            X_min = X[:,i].min()\n",
    "            X_max = X[:,i].max()\n",
    "            step_size = (X_max-X_min) / numSteps\n",
    "            for j in range(-1, int(numSteps) + 1):\n",
    "                for ori in [\"left\", \"right\"]:\n",
    "                    thr = X_min + j * step_size\n",
    "                    weight_e, Gx = self.calc_e(X, y, i, thr, ori, D)\n",
    "                    if weight_e < weight_e_min:\n",
    "                        weight_e_min = weight_e\n",
    "                        best_stump[\"e\"] = weight_e_min\n",
    "                        best_stump[\"threshold\"] = thr\n",
    "                        best_stump[\"orientation\"] = ori\n",
    "                        best_stump[\"Gx\"] = Gx\n",
    "                        best_stump[\"feature\"] = i\n",
    "        return best_stump\n",
    "\n",
    "    def adaboost_classfier(self, X, y, max_iter = 200):\n",
    "        m, n = np.shape(X)\n",
    "        # 初始化样本权值\n",
    "        D = np.mat([1 / m] * m)\n",
    "        # 存储每个弱分类器\n",
    "        weak_classfier = []\n",
    "        fx = np.mat(np.zeros((m,1)))\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            stump = self.build_stump(X, y, D)\n",
    "            Gx = stump[\"Gx\"]\n",
    "            \n",
    "            # 公式8.2，计算alpha\n",
    "            alpha = 1/2 * np.log((1 - stump[\"e\"]) / stump[\"e\"])\n",
    "            \n",
    "            # 公式8.4，更新样本权值\n",
    "            D = np.multiply(D, np.exp(-1 * alpha * np.multiply(y, stump[\"Gx\"]).T))\n",
    "            D = D / np.sum(D)\n",
    "            \n",
    "            stump[\"alpha\"] = alpha\n",
    "            weak_classfier.append(stump)\n",
    "    \n",
    "            # 构建线性组合分类器\n",
    "            fx += np.multiply(alpha, Gx)\n",
    "            # 计算测试误差，为0则结束迭代\n",
    "            error = np.sum(np.sign(fx) != y)\n",
    "            if error == 0: break\n",
    "        return weak_classfier\n",
    "\n",
    "\n",
    "    def predict(self, X_test, classfier):\n",
    "        for stump in classfier:\n",
    "            threshold = stump[\"threshold\"]\n",
    "            orientation = stump[\"orientation\"]\n",
    "            feature = stump[\"feature\"]\n",
    "\n",
    "            if orientation == \"left\":\n",
    "                if X_test[:,feature] <= threshold:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return -1\n",
    "            else:\n",
    "                if X_test[:,feature] > threshold:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练集\n",
    "\n",
    "《统计学习方法习题8.1》为例，用AdaBoost算法学习一个强分类器（从结果中可看出，迭代6次就可以结束了）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.mat([[0.,1.,3.],\n",
    "                  [0.,3.,1.],\n",
    "                  [1.,2.,2.],\n",
    "                  [1.,1.,3.],\n",
    "                  [1.,2.,3.],\n",
    "                  [0.,1.,2.],\n",
    "                  [1.,1.,2.],\n",
    "                  [1.,1.,1.],\n",
    "                  [1.,3.,1.],\n",
    "                  [0.,2.,1.]])\n",
    "train_y = np.mat([-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0]).T\n",
    "adaboost = Adaboost()\n",
    "tree = adaboost.adaboost_classfier(X=train_X, y=train_y, max_iter = 50)\n",
    "len(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
